---
layout: post
title: "LLMs: From having thoughts to managing them"
slug: llms-and-thoughts
tags:
- large-language-models
- llms
- thinking
- vibes
description: "Large-language models don't just automate work for us, they take away the bad vibes caused by thinking too hard."
---
Opening caveat before I begin: I don't know anything about cognitive science (and I don't really know much about anything else either). But I definitely know the vibes I feel when I work in big, complex systems and do analytical work. So this is a post about my own vibes, really.

So to start, I don't get up in the morning excited to work with large-language models (LLMs). That's something a friend of mine Lukas first said of themselves and its become a reflection of my own ever since. I simply love developing an understanding of and relationship with a system. Part of what I love about technical work is a sense of control and influence.

(That being said, I anticipate that eventually anyone who doesn't have expertise with them will end up as a low level worker who has to report to one. So for now, they're something I experiment with.)

But I read a recent [provocation/post by Hullman](https://statmodeling.stat.columbia.edu/2024/06/24/forking-paths-in-llms-for-data-analysis/) exploring/poking at what sort of balance makes sense between human domain knowledge and machine assistance. In it, Hullman compares different types of analytical help as interventions from either a rule-based assistant, a human stats consultant, or an LLM that works like the human consultant. A major point in the piece is that there is a gray-area level of problem-solving that we often offload onto computational systems (LLMs, logic-based agents, interfaces, or otherwise). The line between human knowledge and machine assistance is fuzzy.

We've had similar discussions in our own lab for the past two years (at least) about levels of abstraction in software. We don't engineer electrical circuitry by hand, let alone actually move electricity by ourselves from one place to another. We rely on the existing computational "stack" where I could write something in typescript, which is transpiled into javascript, which is parsed by a browser, which then passes stuff down all the way through assembly and down to our physical machine in order to perform a vast amount of electrical operations in a matter of mere milliseconds. So does an LLM, as a new layer of abstraction, really matter? Scripting (non-compiled) languages in particular had a strong resistance when they first came about. But now javascript and python rule the world we live in. So is resistance to LLMs really just a repeating example of lower-level engineers and workers getting frustrated that their level of abstraction is being replaced?

Perhaps. But like I said at the start, I understand my own vibes on this at least. So, I'll start there by just admitting that this critique (and Hullman's provocation) are very likely to set the prevailing interpretations of people like me who resist LLMs.

But if I'm going down in history as a luddite or perhaps at best just irrational about where I think "good" automation is in computational interaction, I suppose I should at least explore my thoughts on this.

And I like how thinking about something feels. To me, that's why I get up in the morning. I love tricky problems, big systems, and hard questions. I don't like them because solving them is satisfying but because *working though them* is. And unlike automation that I write myself to alleviate tedium for my own tasks (or automation I've never thought about like however assembly actually works when I write javascript), using an LLM to do certain work for me feels like lost opportunities to try to understand something. I miss out on the experience of my own thoughts forming in a way that other kinds of automation don't evoke.

This brings me to the same reason that I don't like generative models that produce art of any kind. At a very personal level, I've been writing fiction pretty much my whole life. I first started forming characters and stories in my head when I was 9, developed a little world when I was 12, and then made the first of 6+ versions of a tabletop rpg where I could explore this world with my friends.

And it would be obscene to me to ask a model to create within this world for me. The joy of creation and forming my thoughts are one of the most precious experiences I have in my life. And the second most precious experience I have in my life is when I share this world and these characters I crafted with my closest friends.

So in the same way, I don't like LLMs for analytical + systems work for the same reason I don't like the idea of generative models taking art from me. I wouldn't ask a robot to dance for me if I wanted to learn how to dance. That's because I want to know how dancing *feels.* I certainly [wouldn't want an AI to go on a first date for me](https://www.wired.com/story/volar-dating-app-chatbot-screen-matches/), I'd want to do that myself. And when I analyze data, build an interface, or engineer some kind of system, I also want to feel a sense of understanding form in my mind and then try to figure out technically how to make that understanding a reality.

I often feel that the "vibes" argument isn't popular with computer and cognitive/behavioral scientists. I understand that a lot of other people simply won't agree or relate at all, which is fine. But tech bros in particular love the managerial vibes that models allow them to have. LLMs, in the Hullman framing, help solve problems but require delegation of tasks. There's some black box effect and trust required, but working with models largely converts work into management.

But I don't like management. And LLMs don't just convert *work* into management, they convert art into management. They convert *thinking* into management. They convert a love of a craft into management. They boil down all the passion and frustration and struggle and joy of creation into tasks for a machine to complete. It's the singularity that capitalism fantasizes about: everyone becomes their own boss of a labor force that never complains.

I see three major players in this game: folks like me (who dislike LLMs for whatever reason), folks who are relatively ambivalent towards them or use them as if they are just some additional tool, and then the ones who are responsible for the everything-ai craze we are now experiencing.

This feral, meteoric rise in popularity for LLMs and generative models is really due to an underlying, massively repressed cultural bedrock beneath a handful of people in the tech world. These dreamers believe in their bones that they should have already become billionares, but simply never got the break they deserved. LLMs offer to deliver on a fantasy that many petite capitalists have been delusional about. They're the ones who really have been driving the LLM revolution forward.

Looking to the future, it's likely bleak. If LLMs don't make it and the bubble bursts, far too many people will be laid off and burnt out as a result. The road will be paved in blood, either way. And if LLMs do survive this bubble and all their promises come true? Then LLMs aren't just coming for our jobs, but our vibes. At best we can dream up the thoughts of management.
